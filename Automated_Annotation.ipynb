{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438c1959-8eb5-412b-8a2b-bd62473adec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Processed 1/92: 1.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Processed 2/92: 100.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Processed 3/92: 103.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Processed 4/92: 104.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Processed 5/92: 105.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Processed 6/92: 106.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Processed 7/92: 107.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Processed 8/92: 108.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "Processed 9/92: 126.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Processed 10/92: 13.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Processed 11/92: 1a5b485681d0a59ce6c4faf95610dcca.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Processed 12/92: 21.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Processed 13/92: 23.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Processed 14/92: 28.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Processed 15/92: 29.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Processed 16/92: 30.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "Processed 17/92: 31.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Processed 18/92: 60.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "Processed 19/92: 61.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Processed 20/92: 63.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Processed 21/92: 65.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Processed 22/92: 66.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Processed 23/92: 67.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Processed 24/92: 7-partial-payment-768x738.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Processed 25/92: 80.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Processed 26/92: 81.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Processed 27/92: 82.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Processed 28/92: 83.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Processed 29/92: 84.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Processed 30/92: 85.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "Processed 31/92: 86.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "Processed 32/92: 87.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Processed 33/92: 89319443_3099124336778208_1718133668842569728_n.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Processed 34/92: 9.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Processed 35/92: 94.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "Processed 36/92: 95.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Processed 37/92: 96.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Processed 38/92: 98.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Processed 39/92: 99.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Processed 40/92: a0f73586855568492d08c109fffba26c.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Processed 41/92: d19a2a2b10ff7dc16fd67e0d4b0e6b1a.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "Processed 42/92: deposit-receipt-b39a92e2e1c31126f8b163aaaa9fad89_og.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Processed 43/92: Dl3d-hcXcAEgA-X.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Processed 44/92: generic-medical-receipt-for-bill-settlement1.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Processed 45/92: hospital-bill-1-788x1023.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Processed 46/92: How-to-print-pay-stubs-in-QuickBooks-Online-1-763x864-1.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Processed 47/92: IMG-20241028-WA0007.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "Processed 48/92: IMG-20241028-WA0008.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Processed 49/92: IMG-20241028-WA0009.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Processed 50/92: IMG-20241028-WA0010.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Processed 51/92: IMG-20241028-WA0011.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Processed 52/92: IMG-20241028-WA0012.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Processed 53/92: IMG-20241028-WA0013.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Processed 54/92: IMG-20241028-WA0014.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Processed 55/92: IMG-20241028-WA0015.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "Processed 56/92: IMG-20241028-WA0016.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Processed 57/92: IMG-20241028-WA0017.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Processed 58/92: IMG-20241028-WA0018.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Processed 59/92: IMG-20241028-WA0019.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Processed 60/92: IMG-20241028-WA0020.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Processed 61/92: IMG-20241028-WA0021.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Processed 62/92: IMG-20241028-WA0022.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Processed 63/92: IMG-20241028-WA0023.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Processed 64/92: IMG-20241028-WA0024.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Processed 65/92: IMG-20241028-WA0025.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Processed 66/92: IMG-20241028-WA0026.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Processed 67/92: IMG-20241028-WA0027.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Processed 68/92: IMG-20241028-WA0028.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Processed 69/92: IMG-20241028-WA0029.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Processed 70/92: IMG-20241028-WA0030.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Processed 71/92: IMG-20241028-WA0031.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Processed 72/92: IMG-20241028-WA0032.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Processed 73/92: IMG-20241028-WA0033.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Processed 74/92: IMG-20241028-WA0034.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Processed 75/92: IMG-20241028-WA0035.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Processed 76/92: IMG-20241028-WA0036.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Processed 77/92: IMG-20241028-WA0037.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Processed 78/92: IMG-20241028-WA0038.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Processed 79/92: IMG-20241028-WA0039.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Processed 80/92: IMG-20241028-WA0040.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Processed 81/92: IMG-20241028-WA0041.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Processed 82/92: invoice_103099-1.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Processed 83/92: large (1).png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Processed 84/92: large (2).png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Processed 85/92: large.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Processed 86/92: Medical-Receipt.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Processed 87/92: OIP (1).jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Processed 88/92: payment-receipt-sample-hospital-bill-format-hospitalbill-768x994.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Processed 89/92: sample-doctor-bill-1-788x1020.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Processed 90/92: Sample-Doctors-Recipt-Template.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Processed 91/92: Screen Shot 2021-07-24 at 2.05.54 AM.png\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Processed 92/92: thumb_1200_1553.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import logging\n",
    "\n",
    "class AutomatedAnnotator:\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = {}\n",
    "        self.annotations_file = os.path.join(image_dir, \"automated_annotations.json\")\n",
    "        \n",
    "       \n",
    "        logging.basicConfig(\n",
    "            filename='annotation_log.txt',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            self.model = ResNet50(weights='imagenet')\n",
    "            logging.info(\"Successfully loaded ResNet50 model\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        \n",
    "        if os.path.exists(self.annotations_file):\n",
    "            with open(self.annotations_file, 'r') as f:\n",
    "                self.annotations = json.load(f)\n",
    "                logging.info(f\"Loaded {len(self.annotations)} existing annotations\")\n",
    "    \n",
    "    def detect_features(self, image_path):\n",
    "        \"\"\"Detect various features in the image using OpenCV\"\"\"\n",
    "        try:\n",
    "            \n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not read image: {image_path}\")\n",
    "            \n",
    "            features = {}\n",
    "            \n",
    "            \n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            \n",
    "            edges = cv2.Canny(gray, 100, 200)\n",
    "            features['edge_density'] = np.mean(edges > 0)\n",
    "            \n",
    "            \n",
    "            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            features['num_contours'] = len(contours)\n",
    "            \n",
    "            \n",
    "            colors = cv2.mean(img)\n",
    "            features['avg_color'] = {\n",
    "                'blue': colors[0],\n",
    "                'green': colors[1],\n",
    "                'red': colors[2]\n",
    "            }\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in detect_features for {image_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def classify_image(self, image_path):\n",
    "        \"\"\"Classify image using ResNet50\"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            img = img.convert('RGB')\n",
    "            img = img.resize((224, 224))\n",
    "            x = np.array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "            \n",
    "            preds = self.model.predict(x)\n",
    "            predictions = decode_predictions(preds, top=3)[0]\n",
    "            \n",
    "            return [{'label': label, 'confidence': float(confidence)} \n",
    "                   for _, label, confidence in predictions]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in classify_image for {image_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_description(self, features, classifications):\n",
    "        \"\"\"Generate a text description based on features and classifications\"\"\"\n",
    "        try:\n",
    "            description = []\n",
    "            \n",
    "            \n",
    "            if classifications:\n",
    "                top_class = classifications[0]['label'].replace('_', ' ')\n",
    "                confidence = classifications[0]['confidence']\n",
    "                description.append(f\"This image appears to be a {top_class} ({confidence:.1%} confidence)\")\n",
    "            \n",
    "            \n",
    "            if features:\n",
    "                \n",
    "                if features['edge_density'] > 0.1:\n",
    "                    description.append(\"The image contains significant detail/edges\")\n",
    "                else:\n",
    "                    description.append(\"The image is relatively smooth\")\n",
    "                \n",
    "                \n",
    "                colors = features['avg_color']\n",
    "                dominant_color = max(colors.items(), key=lambda x: x[1])[0]\n",
    "                description.append(f\"The dominant color tone is {dominant_color}\")\n",
    "                \n",
    "                \n",
    "                if features['num_contours'] > 50:\n",
    "                    description.append(\"The image is complex with many distinct shapes\")\n",
    "                else:\n",
    "                    description.append(\"The image has a simple composition\")\n",
    "            \n",
    "            return \" \".join(description)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in generate_description: {str(e)}\")\n",
    "            return \"Could not generate description\"\n",
    "    \n",
    "    def process_directory(self):\n",
    "        \"\"\"Process all images in the directory\"\"\"\n",
    "        image_files = [f for f in os.listdir(self.image_dir) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        total_files = len(image_files)\n",
    "        logging.info(f\"Starting to process {total_files} images\")\n",
    "        \n",
    "        for i, image_file in enumerate(image_files, 1):\n",
    "            try:\n",
    "                image_path = os.path.join(self.image_dir, image_file)\n",
    "                \n",
    "                \n",
    "                if image_file in self.annotations:\n",
    "                    logging.info(f\"Skipping {image_file} - already annotated\")\n",
    "                    continue\n",
    "                \n",
    "                logging.info(f\"Processing image {i}/{total_files}: {image_file}\")\n",
    "                \n",
    "                \n",
    "                features = self.detect_features(image_path)\n",
    "                \n",
    "                \n",
    "                classifications = self.classify_image(image_path)\n",
    "                \n",
    "                \n",
    "                description = self.generate_description(features, classifications)\n",
    "                \n",
    "                \n",
    "                self.annotations[image_file] = {\n",
    "                    'timestamp': datetime.datetime.now().isoformat(),\n",
    "                    'features': features,\n",
    "                    'classifications': classifications,\n",
    "                    'description': description,\n",
    "                    'automated': True\n",
    "                }\n",
    "                \n",
    "                \n",
    "                if i % 10 == 0:\n",
    "                    self.save_annotations()\n",
    "                    \n",
    "                print(f\"Processed {i}/{total_files}: {image_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {image_file}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        self.save_annotations()\n",
    "        logging.info(\"Finished processing all images\")\n",
    "    \n",
    "    def save_annotations(self):\n",
    "        \"\"\"Save annotations to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.annotations_file, 'w') as f:\n",
    "                json.dump(self.annotations, f, indent=4)\n",
    "            logging.info(f\"Saved annotations to {self.annotations_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving annotations: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = r\"C:\\Users\\eKasi_SWT_COM00862\\Downloads\\doctor\\doctor\"\n",
    "    annotator = AutomatedAnnotator(image_dir)\n",
    "    annotator.process_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ac79a-d016-47a1-8953-12186f83b66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
