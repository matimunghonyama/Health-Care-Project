{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5eaec73-7993-4a92-bc80-78104ecae69a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:20\u001b[1;36m\u001b[0m\n\u001b[1;33m    def load_data(self):\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "class AnnotationAnalyzer:\n",
    "    def __init__(self, annotation_path, image_dir, min_samples_per_class=2):\n",
    "        self.annotation_path = annotation_path\n",
    "        self.image_dir = image_dir\n",
    "        self.min_samples_per_class = min_samples_per_class\n",
    "        self.df = None\n",
    "        self.load_data()\n",
    "\n",
    "     def load_data(self):\n",
    "        \"\"\"Load and preprocess annotation data\"\"\"\n",
    "        \n",
    "        with open(self.annotation_path, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "            \n",
    "        \n",
    "        records = []\n",
    "        for filename, data in annotations.items():\n",
    "            record = {\n",
    "                'filename': filename,\n",
    "                'timestamp': datetime.fromisoformat(data['timestamp']),\n",
    "                'description': data['description']\n",
    "            }\n",
    "            \n",
    "           \n",
    "            if data.get('classifications'):\n",
    "                record['top_class'] = data['classifications'][0]['label']\n",
    "                record['confidence'] = data['classifications'][0]['confidence']\n",
    "            \n",
    "           \n",
    "            if data.get('features'):\n",
    "                record['edge_density'] = data['features']['edge_density']\n",
    "                record['num_contours'] = data['features']['num_contours']\n",
    "                for color, value in data['features']['avg_color'].items():\n",
    "                    record[f'avg_{color}'] = value\n",
    "            \n",
    "            records.append(record)\n",
    "        \n",
    "        self.df = pd.DataFrame(records)\n",
    "        \n",
    "        \n",
    "        class_counts = self.df['top_class'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= self.min_samples_per_class].index\n",
    "        self.df = self.df[self.df['top_class'].isin(valid_classes)]\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} annotations after filtering\")\n",
    "        print(f\"Number of unique classes: {len(valid_classes)}\")\n",
    "    \n",
    "    def build_model(self, num_classes):\n",
    "        \"\"\"Build and compile the model with ResNet50 base\"\"\"\n",
    "        base_model = ResNet50(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def prepare_training_data(self, target_size=(224, 224)):\n",
    "        \"\"\"Prepare data for model training\"\"\"\n",
    "        self.analyze_class_distribution()\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        self.df['label_encoded'] = le.fit_transform(self.df['top_class'])\n",
    "        \n",
    "        train_df, val_df = train_test_split(\n",
    "            self.df, \n",
    "            test_size=0.2, \n",
    "            stratify=self.df['label_encoded'] if len(self.df['top_class'].unique()) > 1 else None,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining set size: {len(train_df)}\")\n",
    "        print(f\"Validation set size: {len(val_df)}\")\n",
    "        \n",
    "        train_generator = self.create_data_generator(train_df, target_size, augment=True)\n",
    "        val_generator = self.create_data_generator(val_df, target_size, augment=False)\n",
    "        \n",
    "        return train_generator, val_generator, le, len(train_df), len(val_df)\n",
    "    \n",
    "    def create_data_generator(self, dataframe, target_size, augment=True):\n",
    "        \"\"\"Create a data generator that yields batches of images and labels\"\"\"\n",
    "        def generate_batches(batch_size=32):\n",
    "            num_samples = len(dataframe)\n",
    "            indices = np.arange(num_samples)\n",
    "            \n",
    "            while True:\n",
    "                np.random.shuffle(indices)\n",
    "                \n",
    "                for start_idx in range(0, num_samples, batch_size):\n",
    "                    end_idx = min(start_idx + batch_size, num_samples)\n",
    "                    batch_indices = indices[start_idx:end_idx]\n",
    "                    \n",
    "                    batch_df = dataframe.iloc[batch_indices]\n",
    "                    \n",
    "                    images = []\n",
    "                    labels = []\n",
    "                    \n",
    "                    for _, row in batch_df.iterrows():\n",
    "                        try:\n",
    "                            img_path = os.path.join(self.image_dir, row['filename'])\n",
    "                            img = cv2.imread(img_path)\n",
    "                            \n",
    "                            if img is None:\n",
    "                                print(f\"Warning: Skipping unreadable image {row['filename']}\")\n",
    "                                continue\n",
    "                                \n",
    "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            img = cv2.resize(img, target_size)\n",
    "                            img = img.astype(np.float32) / 255.0\n",
    "                            \n",
    "                            if augment:\n",
    "                                # Add augmentation if needed\n",
    "                                pass\n",
    "                            \n",
    "                            images.append(img)\n",
    "                            labels.append(row['label_encoded'])\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {row['filename']}: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if images:  # Only yield if we have valid images\n",
    "                        yield np.array(images), np.array(labels)\n",
    "        \n",
    "        return generate_batches\n",
    "    \n",
    "    def train_model(self, epochs=10, batch_size=32):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        train_gen, val_gen, label_encoder, train_size, val_size = self.prepare_training_data()\n",
    "        num_classes = len(label_encoder.classes_)\n",
    "        \n",
    "        print(f\"\\nTraining model with {num_classes} classes\")\n",
    "        \n",
    "        model = self.build_model(num_classes)\n",
    "        \n",
    "        steps_per_epoch = max(1, train_size // batch_size)\n",
    "        validation_steps = max(1, val_size // batch_size)\n",
    "        \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'best_model.keras',\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Now we call the generator without passing size parameters\n",
    "        history = model.fit(\n",
    "            train_gen(batch_size),\n",
    "            validation_data=val_gen(batch_size),\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.plot_training_history(history)\n",
    "        return model, label_encoder, history\n",
    "\n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        plt.close()\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    annotation_path = r\"C:\\Users\\eKasi_SWT_COM00862\\Desktop\\Health_Care_Project\\automated_annotations.json\"\n",
    "    image_dir = r\"C:\\Users\\eKasi_SWT_COM00862\\Desktop\\Health_Care_Project\\doctor\"\n",
    "    \n",
    "    analyzer = AnnotationAnalyzer(annotation_path, image_dir, min_samples_per_class=2)\n",
    "    model, label_encoder, history = analyzer.train_model(epochs=10)\n",
    "    \n",
    "    # Save the model with proper extension\n",
    "    save_dir = r\"C:\\Users\\eKasi_SWT_COM00862\\Desktop\\Health_Care_Project\"\n",
    "    model.save(os.path.join(save_dir, 'trained_model.keras'))\n",
    "    \n",
    "    # Save the label encoder\n",
    "    with open(os.path.join(save_dir, 'label_encoder.pkl'), 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0550c-16bd-44ae-a733-a213813e1e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
